#!/usr/bin/env python3
#
# Output of Brainome Daimensions(tm) Table Compiler v0.5.
# Compile time: Feb-16-2020 13:33:13
# Invocation: btc -v -v spectrometer-1.csv -o spectrometer-1.py
# This source code requires Python 3.
#
"""
System Type:                        Binary classifier
Best-guess accuracy:                89.64%
Model accuracy:                     98.49% (523/531 correct)
Improvement over best guess:        8.85% (of possible 10.36%)
Model capacity (MEC):               33 bits
Generalization ratio:               15.84 bits/bit
Model efficiency:                   0.26%/parameter
System behavior
True Negatives:                     88.51% (470/531)
True Positives:                     9.98% (53/531)
False Negatives:                    0.38% (2/531)
False Positives:                    1.13% (6/531)
True Pos. Rate/Sensitivity/Recall:  0.96
True Neg. Rate/Specificity:         0.99
Precision:                          0.90
F-1 Measure:                        0.93
False Negative Rate/Miss Rate:      0.04
Critical Success Index:             0.87
Model bias:                         100.00% higher chance to pick class 0
"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF=100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE="spectrometer-1.csv"


#Number of output logits
num_output_logits = 1

#Number of attributes
num_attr = 102

mappings = [{3728028160.0: 0, 3461658625.0: 1, 3567083521.0: 2, 831946241.0: 3, 2708039104.0: 4, 571659272.0: 5, 3980022793.0: 6, 1105624585.0: 7, 633491979.0: 8, 2789649932.0: 9, 862180365.0: 10, 4199748617.0: 11, 3231424525.0: 12, 3006915604.0: 13, 4136539668.0: 14, 2502085655.0: 15, 2190445080.0: 16, 2979361307.0: 17, 731596317.0: 18, 879679011.0: 19, 922764325.0: 20, 753464870.0: 21, 1141691946.0: 22, 1863470125.0: 23, 2092076078.0: 24, 1156580910.0: 25, 2824560688.0: 26, 3725504054.0: 27, 3626635319.0: 28, 4277166646.0: 29, 1391571512.0: 30, 3850278977.0: 31, 199859779.0: 32, 343713860.0: 33, 2953483848.0: 34, 4175946824.0: 35, 345464906.0: 36, 3136145483.0: 37, 3351278157.0: 38, 630853198.0: 39, 1449238608.0: 40, 3673344594.0: 41, 1881635922.0: 42, 184456274.0: 43, 709066325.0: 44, 262013015.0: 45, 583397464.0: 46, 3982323801.0: 47, 1508396632.0: 48, 2515765850.0: 49, 4211735132.0: 50, 1719291999.0: 51, 119792225.0: 52, 848931938.0: 53, 2905552483.0: 54, 3732007525.0: 55, 2728369254.0: 56, 2752984677.0: 57, 2583614056.0: 58, 2098253417.0: 59, 3827057765.0: 60, 3832257131.0: 61, 23348331.0: 62, 1562243179.0: 63, 1244904046.0: 64, 1882352747.0: 65, 2856833130.0: 66, 3745353841.0: 67, 3755886702.0: 68, 915608694.0: 69, 4092606073.0: 70, 854421628.0: 71, 3884485244.0: 72, 936496254.0: 73, 842151872.0: 74, 2270076545.0: 75, 1684816005.0: 76, 2786617479.0: 77, 3230561416.0: 78, 649794698.0: 79, 2329159818.0: 80, 2076523148.0: 81, 1274939019.0: 82, 2361688722.0: 83, 786791059.0: 84, 3703763090.0: 85, 2282247827.0: 86, 86774930.0: 87, 1984268439.0: 88, 4109785748.0: 89, 2536171673.0: 90, 3113233047.0: 91, 334022802.0: 92, 4018219678.0: 93, 2719997088.0: 94, 2951175841.0: 95, 414801568.0: 96, 662336163.0: 97, 2984729252.0: 98, 875610277.0: 99, 2619642019.0: 100, 1903336100.0: 101, 2855606440.0: 102, 2969996456.0: 103, 38602410.0: 104, 4252952229.0: 105, 2291138732.0: 106, 1452183213.0: 107, 711951533.0: 108, 1781428909.0: 109, 2480749744.0: 110, 2094614699.0: 111, 2438648499.0: 112, 2895931572.0: 113, 2416361653.0: 114, 3269980854.0: 115, 3659864761.0: 116, 2653350074.0: 117, 831022265.0: 118, 1615741628.0: 119, 3573867197.0: 120, 3651495614.0: 121, 2964625084.0: 122, 1480841914.0: 123, 3153408193.0: 124, 804373186.0: 125, 3318383810.0: 126, 2654935236.0: 127, 384396485.0: 128, 38061252.0: 129, 3635906240.0: 130, 1393421002.0: 131, 3987170506.0: 132, 1950260940.0: 133, 1260595919.0: 134, 3935874256.0: 135, 1199699668.0: 136, 4230439124.0: 137, 962711257.0: 138, 1894440665.0: 139, 2857146073.0: 140, 1899934940.0: 141, 125509343.0: 142, 3006068448.0: 143, 2971263201.0: 144, 1336234720.0: 145, 1927380194.0: 146, 2397504735.0: 147, 2228873448.0: 148, 1269561068.0: 149, 620562669.0: 150, 2223856877.0: 151, 535239784.0: 152, 188006130.0: 153, 2436199156.0: 154, 4288806133.0: 155, 1424922869.0: 156, 1993089788.0: 157, 1408720125.0: 158, 2194846467.0: 159, 1595558148.0: 160, 1131417354.0: 161, 3651309323.0: 162, 2773419789.0: 163, 3465069838.0: 164, 2680256784.0: 165, 2769241875.0: 166, 1819090709.0: 167, 3299207448.0: 168, 1460941593.0: 169, 4186584856.0: 170, 1478417691.0: 171, 724481309.0: 172, 2418657568.0: 173, 2281594658.0: 174, 3682419490.0: 175, 233250084.0: 176, 218378534.0: 177, 1010739497.0: 178, 910723369.0: 179, 2470243115.0: 180, 2278958379.0: 181, 1315346733.0: 182, 1297804077.0: 183, 1818802479.0: 184, 3684409129.0: 185, 174415665.0: 186, 1629745460.0: 187, 1149631284.0: 188, 2820857143.0: 189, 546982200.0: 190, 425571129.0: 191, 3794140985.0: 192, 299341624.0: 193, 4072337212.0: 194, 2424380733.0: 195, 3084341057.0: 196, 4000454467.0: 197, 430526275.0: 198, 354641221.0: 199, 3712367430.0: 200, 2361664330.0: 201, 3124675403.0: 202, 2502736717.0: 203, 1739640145.0: 204, 1359037266.0: 205, 1265048915.0: 206, 2176499031.0: 207, 3011145050.0: 208, 1732350810.0: 209, 3034530140.0: 210, 4264802653.0: 211, 1224969054.0: 212, 1561602912.0: 213, 2175103328.0: 214, 614842720.0: 215, 1609905507.0: 216, 1615788900.0: 217, 636665188.0: 218, 1886195044.0: 219, 423762791.0: 220, 2923524600.0: 221, 1510126952.0: 222, 91012968.0: 223, 109300583.0: 224, 1895863148.0: 225, 3030283117.0: 226, 3389010798.0: 227, 1698629485.0: 228, 3316397423.0: 229, 2600393076.0: 230, 1937706868.0: 231, 1856086907.0: 232, 2362040188.0: 233, 751524220.0: 234, 4292300670.0: 235, 2069122429.0: 236, 1471951232.0: 237, 1721079684.0: 238, 2042547590.0: 239, 2733154694.0: 240, 1680930698.0: 241, 273067914.0: 242, 977888143.0: 243, 3922925967.0: 244, 298045331.0: 245, 1552744342.0: 246, 2396958102.0: 247, 4135130014.0: 248, 3381918623.0: 249, 1949148063.0: 250, 780198305.0: 251, 1393147811.0: 252, 2344207782.0: 253, 1603057064.0: 254, 2638892969.0: 255, 1408204714.0: 256, 2831932330.0: 257, 1040496559.0: 258, 4145942959.0: 259, 3848971698.0: 260, 384751539.0: 261, 3605624243.0: 262, 3434765236.0: 263, 3088637367.0: 264, 161349050.0: 265, 3368260257.0: 266, 383836606.0: 267, 3281381822.0: 268, 1791979456.0: 269, 1427595713.0: 270, 2219240384.0: 271, 296913859.0: 272, 654412227.0: 273, 1264614336.0: 274, 3861968832.0: 275, 1101459911.0: 276, 2037419976.0: 277, 4167606728.0: 278, 1856400329.0: 279, 3388509122.0: 280, 1258999759.0: 281, 3607546831.0: 282, 1290498000.0: 283, 3340344274.0: 284, 1762808275.0: 285, 3411681748.0: 286, 1728978383.0: 287, 3944441814.0: 288, 2821907415.0: 289, 1485031896.0: 290, 2367013849.0: 291, 384681946.0: 292, 3714043866.0: 293, 2367641050.0: 294, 25559033.0: 295, 825063393.0: 296, 1405550051.0: 297, 2041355748.0: 298, 3673652709.0: 299, 3979114470.0: 300, 4147344872.0: 301, 3045336044.0: 302, 800857581.0: 303, 3131553774.0: 304, 2424533487.0: 305, 3367012336.0: 306, 168132081.0: 307, 4022765042.0: 308, 1755482099.0: 309, 2936962036.0: 310, 4114051570.0: 311, 2698538993.0: 312, 1213678071.0: 313, 3319384568.0: 314, 2987866105.0: 315, 2585309694.0: 316, 3266496511.0: 317, 1788175167.0: 318, 3262722028.0: 319, 3743009523.0: 320, 3852017345.0: 321, 1716134778.0: 322, 3697331230.0: 323, 418076049.0: 324, 545932207.0: 325, 1457526258.0: 326, 3944322266.0: 327, 2135261105.0: 328, 2443967942.0: 329, 362813769.0: 330, 3720994225.0: 331, 1887859267.0: 332, 2174355332.0: 333, 1834061884.0: 334, 2550096478.0: 335, 3744073227.0: 336, 1243125259.0: 337, 3477357573.0: 338, 2263632572.0: 339, 4014819284.0: 340, 690353904.0: 341, 2147525791.0: 342, 1471572028.0: 343, 2983769086.0: 344, 3417670682.0: 345, 3376738937.0: 346, 106552395.0: 347, 1268105659.0: 348, 4058056337.0: 349, 3325152634.0: 350, 1081618286.0: 351, 3029812027.0: 352, 3017169328.0: 353, 2540679764.0: 354, 2507810812.0: 355, 1515413251.0: 356, 1408191589.0: 357, 208944628.0: 358, 2026356848.0: 359, 3529401203.0: 360, 3933136696.0: 361, 558963494.0: 362, 181454204.0: 363, 3391781255.0: 364, 2273387291.0: 365, 541035355.0: 366, 3934068655.0: 367, 487995803.0: 368, 3476609215.0: 369, 3426259749.0: 370, 1996580163.0: 371, 3091056213.0: 372, 160734547.0: 373, 3202314838.0: 374, 1511306530.0: 375, 588640262.0: 376, 1244067738.0: 377, 2687615507.0: 378, 541017236.0: 379, 2324601323.0: 380, 1419865583.0: 381, 2030680114.0: 382, 2345832027.0: 383, 1359805545.0: 384, 2251851080.0: 385, 109768202.0: 386, 2302091470.0: 387, 510886413.0: 388, 3361705054.0: 389, 1143463705.0: 390, 3851346818.0: 391, 3764136716.0: 392, 2635085564.0: 393, 546854341.0: 394, 2510976615.0: 395, 153011272.0: 396, 2042088898.0: 397, 558431427.0: 398, 3687922345.0: 399, 1233709753.0: 400, 4037755128.0: 401, 1814597409.0: 402, 2692450117.0: 403, 3674169666.0: 404, 2969458129.0: 405, 2920429991.0: 406, 378598744.0: 407, 2608602717.0: 408, 2716998122.0: 409, 1413277611.0: 410, 1603258338.0: 411, 4087318976.0: 412, 2050503892.0: 413, 3335961797.0: 414, 3444870002.0: 415, 4118175972.0: 416, 1300873656.0: 417, 3380194261.0: 418, 1330520959.0: 419, 1831645480.0: 420, 1679215821.0: 421, 4163010534.0: 422, 1982069323.0: 423, 2140923543.0: 424, 221170312.0: 425, 1295885240.0: 426, 2828820373.0: 427, 4004856071.0: 428, 2786005050.0: 429, 1431292245.0: 430, 1674949354.0: 431, 2009607175.0: 432, 3965548273.0: 433, 3435832325.0: 434, 1040819615.0: 435, 1235652190.0: 436, 1464700339.0: 437, 1834866438.0: 438, 1929408257.0: 439, 2587214501.0: 440, 4097233706.0: 441, 1925988044.0: 442, 2416279450.0: 443, 1813264444.0: 444, 1542443272.0: 445, 775346484.0: 446, 2090307474.0: 447, 133828950.0: 448, 1209866743.0: 449, 1679892458.0: 450, 1001744452.0: 451, 2585578819.0: 452, 2468405095.0: 453, 1318518149.0: 454, 184695789.0: 455, 463700242.0: 456, 1824345750.0: 457, 2025116234.0: 458, 547010295.0: 459, 491629898.0: 460, 3726682671.0: 461, 3750496083.0: 462, 3852185910.0: 463, 3519722214.0: 464, 359038278.0: 465, 448390488.0: 466, 841137832.0: 467, 759173473.0: 468, 2441440671.0: 469, 3087066200.0: 470, 2769554661.0: 471, 3316738094.0: 472, 3417868334.0: 473, 363773262.0: 474, 1954166276.0: 475, 3232540693.0: 476, 3414256357.0: 477, 1085196.0: 478, 4010025716.0: 479, 3404292542.0: 480, 779766362.0: 481, 802166244.0: 482, 3700004288.0: 483, 4108641434.0: 484, 3039310747.0: 485, 1009782162.0: 486, 1687658496.0: 487, 3750703687.0: 488, 1098960550.0: 489, 3527347559.0: 490, 366727047.0: 491, 3783278189.0: 492, 1015874009.0: 493, 2626371972.0: 494, 118621575.0: 495, 2022666289.0: 496, 3492484198.0: 497, 2798130732.0: 498, 1147670791.0: 499, 2082277058.0: 500, 171574187.0: 501, 1782605130.0: 502, 1502048324.0: 503, 364349170.0: 504, 196898627.0: 505, 913866012.0: 506, 4170640359.0: 507, 3754660787.0: 508, 3920616253.0: 509, 2003270807.0: 510, 2007554972.0: 511, 2151719954.0: 512, 4282390830.0: 513, 3541378818.0: 514, 84852501.0: 515, 4039526604.0: 516, 2863695910.0: 517, 3332840706.0: 518, 1834434176.0: 519, 1905811453.0: 520, 2507175545.0: 521, 3541570877.0: 522, 547587037.0: 523, 611786515.0: 524, 4205653237.0: 525, 1132867658.0: 526, 4242160080.0: 527, 644286260.0: 528, 1395902443.0: 529, 2908668711.0: 530}]
list_of_cols_to_normalize = [0]

transform_true = True

def column_norm(column,mappings):
    listy = []
    for i,val in enumerate(column.reshape(-1)):
        if not (val in mappings):
            mappings[val] = int(max(mappings.values()))+1
        listy.append(mappings[val])
    return np.array(listy)

def Normalize(data_arr):
    if list_of_cols_to_normalize:
        for i,mapping in zip(list_of_cols_to_normalize,mappings):
            if i>=data_arr.shape[1]:
                break
            col = data_arr[:,i]
            normcol = column_norm(col,mapping)
            data_arr[:,i] = normcol
        return data_arr
    else:
        return data_arr

def transform(X):
    mean = None
    components = None
    whiten = None
    explained_variance = None
    if (transform_true):
        mean = np.array([158.5, 2.449685534591195, 18.005122641509434, -10.152066037735846, 0.0, 828.6320754716982, 860.6509433962265, 473.2138364779874, 505.87106918238993, 12499.70407342768, 12230.604419496853, 11992.236893081756, 11562.221435534602, 11139.298971698108, 10856.630129874218, 10471.045083647794, 10341.628637106915, 10233.00739937107, 10144.582082169813, 10058.105925471698, 10078.76703251573, 10036.575027672952, 9796.408087327045, 9557.991257154086, 9213.658567213835, 9006.466217264146, 8770.697651572335, 8500.668542452822, 8298.080747641512, 8135.740477248424, 7864.098644622637, 7637.965282515716, 7419.601235660377, 7110.206179937112, 6915.607956289304, 6598.011614150942, 6391.203945283021, 6085.7830534591185, 5854.551259748425, 5585.49447421384, 5320.231474213836, 5029.445747798743, 4826.227892138365, 4595.820401257857, 4453.398939622641, 4358.485995283017, 4206.351212578615, 4062.9719915094347, 3998.322160377362, 3849.97641509434, 3726.293012893083, 3636.3534232704424, 3538.2693959119524, 8030.732540682392, 7483.774282547165, 6823.180189937108, 6193.102223899377, 5444.014241823902, 4821.002488364779, 4341.820583647798, 3971.5330792452846, 3642.631366666667, 3384.104109748429, 3159.1385628930816, 2999.805420754719, 2827.0716984276737, 2732.208795283019, 2604.273921383649, 2575.1439663522015, 2508.3464569182393, 2460.3862402515715, 2393.0054789308165, 2374.6258216981137, 2337.516435125785, 2294.3728370125773, 2233.353575817609, 2189.6838182075476, 2142.5057611635216, 2080.109304779875, 2042.3684130817612, 1976.5735672012574, 1933.4333143396232, 1878.5400549685542, 1823.0285983647793, 1771.7712827358496, 1702.7918167295609, 1683.706118396227, 1616.6047715723273, 1535.535955062893, 1552.8828295283017, 1485.669201446541, 1410.9557594339624, 1428.9726040251574, 1411.3548195283024, 1313.3987595597484, 1274.274691603773, 1292.4647963773577, 1253.5291337106921, 1219.2132319496848, 1198.5095715408806, 1137.231874198113, 1020.9569044339623])
        components = np.array([array([-4.95471408e-04,  2.14667435e-05, -2.15241013e-06,  3.53597296e-04,
        0.00000000e+00, -1.18874983e-02, -1.21770977e-02, -6.94234157e-03,
       -8.12335588e-03,  4.52949624e-01,  4.24148879e-01,  3.90351102e-01,
        3.46655048e-01,  2.98037044e-01,  2.43648919e-01,  1.92479416e-01,
        1.43132546e-01,  8.99394933e-02,  4.20305563e-02, -2.86762792e-03,
       -3.40946861e-02, -5.64049912e-02, -7.22483280e-02, -8.09374501e-02,
       -8.32966433e-02, -8.35219742e-02, -7.93171137e-02, -7.83003973e-02,
       -7.27273784e-02, -6.57942926e-02, -6.01961660e-02, -5.63412094e-02,
       -5.15282774e-02, -5.07690609e-02, -4.44267071e-02, -3.81688528e-02,
       -3.47694440e-02, -2.75767806e-02, -2.23393779e-02, -1.68123515e-02,
       -9.18054373e-03, -7.08097111e-03, -4.11094689e-03, -4.17254273e-03,
       -8.04760060e-04, -2.53293771e-04,  5.17781250e-04, -2.56838779e-04,
        1.26279626e-04,  6.90601638e-04,  1.44287066e-03,  2.88312820e-03,
        2.57667996e-03, -6.16388684e-02, -5.18282988e-02, -4.04777169e-02,
       -2.69559232e-02, -1.43939008e-02, -5.95551786e-03, -2.52535075e-03,
       -1.14771478e-04, -3.04510589e-03, -3.53078505e-03, -5.13266971e-03,
       -7.58477540e-03, -1.05475851e-02, -1.25944849e-02, -1.87016114e-02,
       -2.27639706e-02, -2.63098839e-02, -3.12201159e-02, -3.44590588e-02,
       -3.74832562e-02, -3.95309680e-02, -4.26521908e-02, -4.45495746e-02,
       -4.39874822e-02, -4.51488125e-02, -4.61148367e-02, -4.63211604e-02,
       -4.48067768e-02, -4.67859249e-02, -4.46106709e-02, -4.27927998e-02,
       -4.22387843e-02, -3.87132163e-02, -3.80446290e-02, -3.73678654e-02,
       -3.59652776e-02, -3.63353963e-02, -3.51879443e-02, -3.40261840e-02,
       -3.38426358e-02, -3.32117293e-02, -3.07385377e-02, -3.04529886e-02,
       -3.02138295e-02, -2.90965208e-02, -2.75487877e-02, -2.68823155e-02,
       -2.55978475e-02, -2.32818739e-02]), array([ 3.15631413e-04, -4.15752882e-05, -2.84725487e-05, -4.32653731e-04,
        1.38777878e-17,  1.01768028e-02,  9.74662000e-03,  7.69093772e-03,
        9.68928786e-03, -2.09390178e-02, -1.69768754e-03, -1.06597728e-03,
       -1.14376258e-02, -2.93654959e-02, -5.38513076e-02, -8.11927147e-02,
       -1.15010966e-01, -1.51628583e-01, -1.84553356e-01, -2.12822534e-01,
       -2.28956508e-01, -2.40207313e-01, -2.41744859e-01, -2.37107496e-01,
       -2.25487062e-01, -2.10318388e-01, -1.91789023e-01, -1.80309726e-01,
       -1.66904257e-01, -1.54880094e-01, -1.38740210e-01, -1.23635607e-01,
       -1.08457664e-01, -9.39526074e-02, -7.66174946e-02, -5.95633571e-02,
       -4.76062019e-02, -3.29855910e-02, -2.23133093e-02, -1.04928363e-02,
        3.15106350e-03,  1.33254660e-02,  2.11290257e-02,  2.73282225e-02,
        3.54314680e-02,  4.21080055e-02,  5.41070895e-02,  5.30371919e-02,
        4.96850479e-02,  4.63950189e-02,  4.35379381e-02,  4.29947250e-02,
        4.80522584e-02, -1.47444781e-01, -1.19446973e-01, -7.90960113e-02,
       -3.80284829e-02,  2.72676997e-04,  2.85041949e-02,  4.50052629e-02,
        5.36421682e-02,  5.78517137e-02,  6.25280326e-02,  6.51240561e-02,
        6.90871315e-02,  7.01170735e-02,  7.22599716e-02,  7.39615324e-02,
        7.55336262e-02,  7.51757703e-02,  7.44652652e-02,  7.61037268e-02,
        7.34801655e-02,  7.35687858e-02,  7.45913861e-02,  7.51334347e-02,
        7.49139588e-02,  7.52717442e-02,  7.69696293e-02,  7.86721905e-02,
        8.34463649e-02,  8.59995640e-02,  8.48376931e-02,  8.54746539e-02,
        8.55443099e-02,  8.65908836e-02,  8.86448062e-02,  9.06398354e-02,
        8.86888309e-02,  9.70436942e-02,  9.38312784e-02,  9.39444887e-02,
        1.01480135e-01,  1.02593258e-01,  9.72229971e-02,  9.86817085e-02,
        1.02374180e-01,  1.03075136e-01,  1.01286328e-01,  1.02192064e-01,
        9.80901407e-02,  8.54532912e-02]), array([-2.21639029e-03,  1.50501818e-04, -2.77631579e-05, -7.49803454e-04,
       -1.11022302e-16, -8.37758002e-03, -7.98228543e-03, -2.39307999e-03,
        1.23959448e-03,  1.77551003e-01,  1.05497793e-01,  3.21359164e-02,
       -2.37648250e-02, -4.05504481e-02, -3.93172624e-02, -3.57085844e-02,
       -1.77578085e-02,  2.44891459e-02,  6.94780306e-02,  1.27299404e-01,
        1.52358620e-01,  1.54078039e-01,  1.50106500e-01,  1.36732175e-01,
        1.13742991e-01,  8.25744554e-02,  6.33724286e-02,  3.25687378e-02,
        6.64592861e-03, -1.70168021e-02, -4.52868032e-02, -6.35679186e-02,
       -7.28342728e-02, -8.71960172e-02, -9.49961088e-02, -1.08871032e-01,
       -1.32837340e-01, -1.43233969e-01, -1.45057814e-01, -1.51902949e-01,
       -1.58249941e-01, -1.56755254e-01, -1.51933981e-01, -1.47229014e-01,
       -1.35276571e-01, -1.21221014e-01, -1.03308633e-01, -1.07752943e-01,
       -1.19472384e-01, -1.24192751e-01, -1.26044708e-01, -1.24246056e-01,
       -1.22448675e-01, -2.18330698e-02, -6.40173814e-02, -1.04930944e-01,
       -1.42387557e-01, -1.52633147e-01, -1.41540987e-01, -1.29974428e-01,
       -1.20756398e-01, -1.11125622e-01, -9.49997884e-02, -8.64011529e-02,
       -8.06969019e-02, -6.99383968e-02, -5.36970029e-02, -3.99749777e-02,
       -2.90314117e-02, -1.01668893e-02, -1.41789528e-03,  1.67403883e-02,
        2.87732690e-02,  3.36073202e-02,  4.07607816e-02,  5.30900958e-02,
        6.14372378e-02,  6.82541774e-02,  7.57042046e-02,  8.37650791e-02,
        9.38672557e-02,  9.76199350e-02,  9.89009592e-02,  9.65662092e-02,
        9.97291207e-02,  1.00335519e-01,  1.04538824e-01,  1.04137322e-01,
        9.94805767e-02,  1.12730971e-01,  1.10183692e-01,  1.16088757e-01,
        1.25760184e-01,  1.25190738e-01,  1.21635234e-01,  1.23699866e-01,
        1.28142294e-01,  1.30969895e-01,  1.31508638e-01,  1.29365742e-01,
        1.24747358e-01,  1.07592836e-01]), array([-1.14003050e-03,  1.84168781e-04,  1.35872372e-04,  2.52877330e-03,
        1.52655666e-16,  7.76131821e-03,  6.27358679e-03,  5.82767668e-03,
        4.00837400e-03,  3.25732119e-01,  2.71991684e-01,  1.45961541e-01,
       -1.99732730e-02, -1.50713466e-01, -2.68784898e-01, -3.21969381e-01,
       -3.54497089e-01, -3.37211201e-01, -2.83085756e-01, -1.83996265e-01,
       -5.40073252e-02,  6.71565137e-02,  1.18339232e-01,  1.61379553e-01,
        1.74996420e-01,  1.58113175e-01,  1.45153703e-01,  9.65383258e-02,
        8.03515704e-02,  4.88900666e-02,  2.24783301e-02, -2.37457123e-04,
       -1.26693962e-02, -3.10883310e-02, -2.87307806e-02, -2.52354986e-02,
       -3.12346270e-02, -2.47478115e-02, -1.75065117e-02, -1.67082242e-02,
        6.94831089e-03,  1.49334961e-02,  3.22597710e-02,  3.79977556e-02,
        5.05682761e-02,  6.06559575e-02,  7.12443563e-02,  7.92355900e-02,
        9.77081743e-02,  9.95472411e-02,  8.82857894e-02,  8.44812247e-02,
        9.24673786e-02,  4.06233660e-02,  2.26122126e-02, -1.43343443e-02,
       -1.96315342e-02, -6.58633473e-03,  1.62302682e-02,  4.26651672e-02,
        6.09402729e-02,  4.81827941e-02,  4.53464837e-02,  3.65447415e-02,
        1.71335363e-02,  9.84973081e-03,  8.12808676e-04, -1.88385225e-02,
       -3.14851512e-02, -2.22674037e-02, -3.20562943e-02, -3.17378310e-02,
       -2.67801607e-02, -2.91732643e-02, -2.74669418e-02, -3.77558370e-02,
       -2.74534628e-02, -2.89267613e-02, -3.28412880e-02, -3.76021363e-02,
       -3.23661778e-02, -3.65448366e-02, -3.23134226e-02, -3.15590796e-02,
       -2.77910378e-02, -2.77029144e-02, -2.00700392e-02, -8.77705394e-03,
       -1.41682092e-02, -1.43413208e-02, -2.18498248e-02, -1.94710027e-02,
       -1.08733740e-02, -1.07953926e-02, -1.34821465e-02, -1.00721525e-02,
       -1.28710644e-02, -1.21825798e-02, -5.87234382e-03, -8.21062180e-03,
       -6.03163600e-03, -9.67620225e-03]), array([ 1.39915708e-03, -5.73298299e-05, -3.29576298e-05, -2.88031007e-04,
        2.77555756e-17,  5.57758133e-02,  5.57609689e-02,  2.71067380e-02,
        2.85229687e-02,  1.97126290e-02,  5.93553623e-02,  1.02871627e-01,
        9.73230142e-02,  5.88411155e-02,  1.09054864e-02, -4.63603197e-02,
       -9.26834569e-02, -1.42584697e-01, -1.65078612e-01, -1.74168677e-01,
       -1.77081985e-01, -1.79321693e-01, -1.49439384e-01, -1.08853167e-01,
       -9.99552634e-02, -4.80526333e-02, -3.49496530e-03,  4.19163721e-02,
        9.35890701e-02,  1.34976576e-01,  1.72625473e-01,  1.96296249e-01,
        2.41034360e-01,  2.17062144e-01,  2.15206913e-01,  1.75815778e-01,
        1.52280052e-01,  1.24201913e-01,  9.98159988e-02,  5.88596705e-02,
        2.76150185e-02, -7.89909943e-03, -2.87452633e-02, -5.36004936e-02,
       -8.07000615e-02, -1.00425742e-01, -9.43007348e-02, -1.22471265e-01,
       -1.53267956e-01, -1.58570004e-01, -1.52076365e-01, -1.55574502e-01,
       -1.50945358e-01,  1.12152705e-01,  1.58446313e-01,  1.80484193e-01,
        1.34803757e-01,  6.06062016e-02, -1.73641881e-02, -7.36886689e-02,
       -1.05452416e-01, -1.20634820e-01, -1.18143566e-01, -1.19288562e-01,
       -1.04565673e-01, -1.03237653e-01, -9.30069250e-02, -8.28133453e-02,
       -8.15112168e-02, -6.84573757e-02, -4.88614660e-02, -4.32407511e-02,
       -3.66688303e-02, -3.05607408e-02, -3.14438605e-02, -1.84198750e-02,
       -1.26149903e-02, -1.91096384e-02, -3.81479377e-03,  2.69445042e-03,
        9.17840157e-03,  5.77522567e-03,  2.28476106e-02,  2.02964251e-02,
        1.26371803e-02,  2.35633400e-02,  2.24173291e-02,  3.39095239e-02,
        3.62646520e-02,  3.35338114e-02,  4.64777079e-02,  5.48251256e-02,
        5.20814611e-02,  6.10956949e-02,  6.68069755e-02,  7.02738255e-02,
        7.18670135e-02,  7.19490202e-02,  8.45022128e-02,  8.53466065e-02,
        8.28750539e-02,  6.05339566e-02]), array([-3.70473786e-03, -4.07548062e-04,  5.52293182e-05, -5.39978050e-04,
        8.32667268e-17,  5.86678213e-01,  5.98716377e-01,  3.50075185e-01,
        3.48528784e-01,  6.97537658e-02, -3.57184162e-03, -2.76923964e-02,
       -3.61874486e-02, -6.31150217e-02, -1.66793118e-02,  1.61826291e-02,
        3.18273772e-02,  7.95252489e-02,  5.49898259e-02,  3.17340239e-02,
        2.15768007e-02,  4.61073684e-03, -6.42539829e-03, -1.16111716e-02,
        1.10444189e-02, -1.63660395e-02,  2.74287095e-05, -2.53734205e-02,
       -2.25773638e-02, -1.61555560e-02, -2.56460105e-02, -1.16365743e-02,
       -3.72854595e-02, -3.61957543e-03, -9.95359835e-03, -1.44021009e-02,
       -2.18546876e-03, -1.65760395e-02, -1.44669377e-02,  1.80615490e-02,
       -3.91623482e-03, -3.25219100e-02, -2.69938022e-02,  1.59277593e-02,
       -9.84186425e-03,  1.48382098e-02,  3.98792465e-02,  2.24722105e-02,
        5.32315271e-02,  1.23867949e-02,  5.62353275e-03,  8.79479210e-03,
        1.39153042e-04,  5.27871473e-02,  1.63050884e-02, -2.75416236e-02,
        1.01777567e-03, -7.88251649e-03, -5.46167818e-03,  2.28765896e-02,
        8.58461382e-03,  3.61107370e-02,  2.38366102e-02,  1.62452437e-02,
        1.43646817e-02, -1.51627522e-02,  1.60285533e-03,  1.99305607e-03,
       -1.01098527e-02, -9.39956566e-03,  4.93599712e-03, -1.97622814e-02,
       -1.95475879e-02, -3.00168898e-02, -1.51562790e-02, -1.50185504e-02,
       -2.20152709e-02, -3.31684596e-02, -8.40959410e-03, -2.11265737e-02,
       -2.52303013e-02, -2.89821944e-02, -2.87257604e-02, -1.05487413e-02,
       -1.68222089e-02, -1.54217663e-02, -9.61080394e-03,  2.40450255e-03,
        7.45851643e-03, -7.95924260e-03,  1.32480508e-03,  3.73356570e-03,
       -5.39956420e-04,  1.28895966e-02,  1.57872705e-03,  5.32277636e-03,
        2.69415535e-02,  1.35516595e-02,  1.64884542e-02,  2.51156121e-02,
        1.64201022e-02,  1.18951623e-02])])
        whiten = False
        explained_variance = np.array([138663682.50363046, 86413900.9933856, 9752362.510481002, 5994329.973265185, 2740288.399984588, 774605.6558623547])
        X = X - mean

    X_transformed = np.dot(X, components.T)
    if whiten:
        X_transformed /= np.sqrt(explained_variance)
    return X_transformed

# Preprocessor for CSV files
def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    
    clean.classlist=[]
    clean.testfile=testfile
    
    def convert(cell):
        value=str(cell)
        try:
            result=int(value)
            return result
        except:
            try:
                result=float(value)
                if (rounding!=-1):
                    result=int(result*math.pow(10,rounding))/math.pow(10,rounding)
                return result
            except:
                result=(binascii.crc32(value.encode('utf8')) % (1<<32))
                return result

    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value=str(cell)
        if (value==''):
            raise ValueError("All cells in the target column need to contain a class label.")
        try:
            result=int(value)
            if (not (result==0 or result==1)):
                raise ValueError("Integer class labels need to be 0 or 1.")
            if (not str(result) in clean.classlist):
                clean.classlist=clean.classlist+[str(result)]
            return result
        except:
            try:
                result=float(value)
                if (rounding!=-1):
                    result=int(result*math.pow(10,rounding))/math.pow(10,rounding)
                if (not (result==0 or result==1)):
                    raise ValueError("Numeric class labels need to be 0 or 1.")
                if (not str(result) in clean.classlist):
                    clean.classlist=clean.classlist+[str(result)]
                return result
            except:
                result=(binascii.crc32(value.encode('utf8')) % (1<<32))
                if (result in clean.classlist):
                    result=clean.classlist.index(result)
                else:
                    clean.classlist=clean.classlist+[result]
                    result=clean.classlist.index(result)
                return result
    rowcount=0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f=open(outfile,"w+")
        if (headerless==False):
            next(reader,None)
        outbuf=[]
        for row in reader:
            if (row==[]):  # Skip empty rows
                continue
            rowcount=rowcount+1
            rowlen=num_attr
            if (not testfile):
                rowlen=rowlen+1    
            if (not len(row)==rowlen):
                raise ValueError("Column count must match trained predictor. Row "+str(rowcount)+" differs.")
            i=0
            for elem in row:
                if(i+1<len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid=str(convertclassid(elem))
                    outbuf.append(classid)
                i=i+1
            if (len(outbuf)<IOBUF):
                outbuf.append("\n")
            else:
                print(''.join(outbuf), file=f)
                outbuf=[]
        print(''.join(outbuf),end="", file=f)
        f.close()

        if (testfile==False and not len(clean.classlist)==2):
            raise ValueError("Number of classes must be 2.")


# Helper (save an import)
def argmax(l):
    f = lambda i: l[i]
    return max(range(len(l)), key=f)

# Classifier
def classify(row):
    x=row
    h_0 = max((((-0.48477787 * float(x[0]))+ (0.369221 * float(x[1]))+ (0.25298357 * float(x[2]))+ (-1.0243605 * float(x[3]))+ (0.2699334 * float(x[4]))+ (-0.14539534 * float(x[5]))) + 8.452563), 0)
    h_1 = max((((0.62672913 * float(x[0]))+ (1.1815501 * float(x[1]))+ (0.34818533 * float(x[2]))+ (-0.079235226 * float(x[3]))+ (0.84116864 * float(x[4]))+ (0.6739708 * float(x[5]))) + -0.052641656), 0)
    h_2 = max((((-6.973235 * float(x[0]))+ (-0.18908171 * float(x[1]))+ (4.9464645 * float(x[2]))+ (-8.2684145 * float(x[3]))+ (2.4792778 * float(x[4]))+ (-2.8749638 * float(x[5]))) + -7.8933206), 0)
    h_3 = max((((-0.16391218 * float(x[0]))+ (-0.53426886 * float(x[1]))+ (0.24300267 * float(x[2]))+ (0.42848977 * float(x[3]))+ (-0.07506324 * float(x[4]))+ (-0.14420754 * float(x[5]))) + 8.918355), 0)
    o_0 = (-2.5788896 * h_0)+ (-1.9815267 * h_1)+ (0.22336616 * h_2)+ (-1.8575083 * h_3) + -8.773168

    if num_output_logits==1:
        return o_0>=0
    else:
        return argmax([eval('o'+str(i)) for i in range(num_output_logits)])

# Main method
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile',action='store_true',help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    args = parser.parse_args()

    if not args.validate: # Then predict
        if not args.cleanfile: # File is not preprocessed
            tempdir=tempfile.gettempdir()
            cleanfile=tempdir+os.sep+"clean.csv"
            clean(args.csvfile,cleanfile, -1, args.headerless, True)
            test_tensor = np.loadtxt(cleanfile,delimiter=',',dtype='float64')
            os.remove(cleanfile)
        else: # File is already preprocessed
            test_tensor = np.loadtxt(args.File,delimiter = ',',dtype = 'float64')               
        test_tensor = Normalize(test_tensor)
        if transform_true:
            test_tensor = transform(test_tensor)
        with open(args.csvfile,'r') as csvinput:
            writer = csv.writer(sys.stdout, lineterminator='\n')
            reader = csv.reader(csvinput)
            if (not args.headerless):
                writer.writerow((next(reader, None)+['Prediction']))
            i=0
            for row in reader:
                if (classify(test_tensor[i])):
                    pred="1"
                else:
                    pred="0"
                row.append(pred)
                writer.writerow(row)
                i=i+1
    elif args.validate: # Then validate this predictor, always clean first.
        tempdir=tempfile.gettempdir()
        temp_name = next(tempfile._get_candidate_names())
        cleanfile=tempdir+os.sep+temp_name
        clean(args.csvfile,cleanfile, -1, args.headerless)
        val_tensor = np.loadtxt(cleanfile,delimiter = ',',dtype = 'float64')
        os.remove(cleanfile)
        val_tensor = Normalize(val_tensor)
        if transform_true:
            trans = transform(val_tensor[:,:-1])
            val_tensor = np.concatenate((trans,val_tensor[:,-1].reshape(-1,1)),axis = 1)
        count,correct_count,num_TP,num_TN,num_FP,num_FN,num_class_1,num_class_0 = 0,0,0,0,0,0,0,0
        for i,row in enumerate(val_tensor):
            if int(classify(val_tensor[i].tolist())) == int(float(val_tensor[i,-1])):
                correct_count+=1
                if int(float(row[-1]))==1:
                    num_class_1+=1
                    num_TP+=1
                else:
                    num_class_0+=1
                    num_TN+=1
            else:
                if int(float(row[-1]))==1:
                    num_class_1+=1
                    num_FN+=1
                else:
                    num_class_0+=1
                    num_FP+=1
            count+=1

        model_cap=33

        FN=float(num_FN)*100.0/float(count)
        FP=float(num_FP)*100.0/float(count)
        TN=float(num_TN)*100.0/float(count)
        TP=float(num_TP)*100.0/float(count)
        num_correct=correct_count

        if int(num_TP+num_FN)!=0:
            TPR=num_TP/(num_TP+num_FN) # Sensitivity, Recall
        if int(num_TN+num_FP)!=0:
            TNR=num_TN/(num_TN+num_FP) # Specificity, 
        if int(num_TP+num_FP)!=0:
            PPV=num_TP/(num_TP+num_FP) # Recall
        if int(num_FN+num_TP)!=0:
            FNR=num_FN/(num_FN+num_TP) # Miss rate
        if int(2*num_TP+num_FP+num_FN)!=0:
            FONE=2*num_TP/(2*num_TP+num_FP+num_FN) # F1 Score
        if int(num_TP+num_FN+num_FP)!=0:
            TS=num_TP/(num_TP+num_FN+num_FP) # Critical Success Index

        randguess=int(float(10000.0*max(num_class_1,num_class_0))/count)/100.0
        modelacc=int(float(num_correct*10000)/count)/100.0

        print("System Type:                        Binary classifier")
        print("Best-guess accuracy:                {:.2f}%".format(randguess))
        print("Model accuracy:                     {:.2f}%".format(modelacc)+" ("+str(int(num_correct))+"/"+str(count)+" correct)")
        print("Improvement over best guess:        {:.2f}%".format(modelacc-randguess)+" (of possible "+str(round(100-randguess,2))+"%)")
        print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
        print("Generalization ratio:               {:.2f}".format(int(float(num_correct*100)/model_cap)/100.0)+" bits/bit")
        print("Model efficiency:                   {:.2f}%/parameter".format(int(100*(modelacc-randguess)/model_cap)/100.0))
        print("System behavior")
        print("True Negatives:                     {:.2f}%".format(TN)+" ("+str(int(num_TN))+"/"+str(count)+")")
        print("True Positives:                     {:.2f}%".format(TP)+" ("+str(int(num_TP))+"/"+str(count)+")")
        print("False Negatives:                    {:.2f}%".format(FN)+" ("+str(int(num_FN))+"/"+str(count)+")")
        print("False Positives:                    {:.2f}%".format(FP)+" ("+str(int(num_FP))+"/"+str(count)+")")
        if int(num_TP+num_FN)!=0:
            print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
        if int(num_TN+num_FP)!=0:
            print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
        if int(num_TP+num_FP)!=0:
            print("Precision:                          {:.2f}".format(PPV))
        if int(2*num_TP+num_FP+num_FN)!=0:
            print("F-1 Measure:                        {:.2f}".format(FONE))
        if int(num_TP+num_FN)!=0:
            print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
        if int(num_TP+num_FN+num_FP)!=0:    
            print("Critical Success Index:             {:.2f}".format(TS))


